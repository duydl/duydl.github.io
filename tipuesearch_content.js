var tipuesearch = {"pages":[{"title":"End-Term Report - GSoC24","text":"This year marks the 20th anniversary of Google Summer of Code (GSoC) , with 195 open-source organizations developing various impactful projects. As part of 1,200 contributors, I had the opportunity to work with Machine Learning for Science (ML4Sci) , an organization that applies state-of-the-art machine learning techniques to solve cutting-edge problems in science. My project, titled \"Quantum Contrastive Representation Learning for High-Energy Physics (HEP),\" involved developing machine learning models—both classical and quantum—to learn useful representations from data for various downstream tasks such as classification, regression, and generation. Project Objectives The focus of this project was to investigate various architectures and pipelines for contrastive learning, as well as the opprotunities and effect of integrating quantum computing methods. The primary pipelines I explored included: Supervised Contrastive Learning : In this method, we attempt to learn useful representations by using a metric instead of, for example, directly predicting a likelihood. The pretrained model is then used to generate embeddings as inputs to simple linear probing methods for downstream tasks. Self-Supervised Contrastive Learning : In this method, we attempt to learn useful representations without labels, where similar and dissimilar pairs are generated using augmentations. The learned representations can then be fine-tuned with labels for downstream tasks like classification. For both pipelines, I explored different types of architectures including Convolutional Neural Networks (CNNs) and Graph Neural Networks (GNNs) . Additionally, I incorporated Quantum Variational Circuits into some of these models to see whether quantum computing could offer an advantage. My work also extended to exploring model-based augmentations for graph data, namely rationale discovery, which is shown to be particularly advantageous with graph distributions that is not understood well enough for effective augmentations. Datasets A significant aspect of the project was working with diverse datasets, each posing its own challenges: Simple Image Classification : I used datasets like MNIST and Fashion MNIST to benchmark initial models, given their simplicity and widespread use in machine learning research. High-Energy Physics (HEP) Datasets : These included images from the LHC experiments , specifically the Track , ECAL , and HCAL components, as well as synthetic particle jets generated by PYTHIA . The Photo-Electron and Quark-Gluon datasets were used for classification tasks, as they represent different particle interactions in the detector. Quantum Machine Molecular Energy (QM7) : For regression tasks, I experimented with the QM7 dataset , which consists of molecules with associated formation energies. This dataset provided an excellent benchmark for testing how well models can learn from small-scale quantum mechanical data. 1. Simple Image Classification Datasets: MNIST & Fashion MNIST To establish a baseline for model performance, I began with simpler, widely-used datasets such as MNIST and Fashion MNIST . These datasets are often used to benchmark image classification models and contrastive learning frameworks due to their accessibility and straightforward structure. MNIST contains 70,000 grayscale images of handwritten digits, each 28x28 pixels, and is used extensively in machine learning research as a starting point for validating models. Fashion MNIST , a similar dataset, consists of 70,000 grayscale images of clothing items. It introduces a slightly higher level of complexity than MNIST, as it involves classifying 10 categories of fashion items rather than digits. These datasets were ideal for experimenting with basic Convolutional Neural Networks (CNNs) and testing out early versions of contrastive learning models. 2. High-Energy Physics (HEP) Datasets A key focus of this project was analyzing complex datasets from high-energy physics (HEP) experiments, particularly from the Large Hadron Collider (LHC) at CERN. The LHC is a massive particle accelerator that collides particles at nearly the speed of light to explore fundamental questions about the universe, such as understanding the building blocks of matter and the forces that govern them. By accelerating particles in a 27-kilometer ring, it allows scientists to study the results of these high-energy collisions, which have led to important discoveries like the Higgs boson . One of the main experiments at the LHC is the CMS (Compact Muon Solenoid) detector. This detector is a large, layered structure designed to track and analyze particles produced in collisions. As particles fly out from the collision point, their trajectories and energies are measured with extreme precision using components such as: - Tracker : Identifies the path of charged particles. - Electromagnetic Calorimeter (ECAL) : Measures the energy of particles like electrons and photons. - Hadron Calorimeter (HCAL) : Detects composite particles, such as quarks and gluons, which interact through the strong force. These components work together to capture detailed data from the collisions, allowing researchers to study the behavior of particles. The challenge is to process this data efficiently and use machine learning to distinguish between different types of particle events. Quark-Gluon Dataset from CMS The Quark-Gluon Dataset is a simulated dataset derived from the data collected by the CMS detector. It consists of over 933,000 images, each 125x125 pixels in size, with three channels that correspond to measurements from the Tracker , ECAL , and HCAL components. Each image represents a snapshot of the energy distribution in a particle collision, and the goal is to classify whether the collision produced a quark-initiated jet or a gluon-initiated jet . Quarks : Fundamental particles that form protons and neutrons, which make up the atomic nucleus. Quarks are the \"building blocks\" of matter. Gluons : Force-carrying particles that mediate the strong force between quarks. They act as the \"glue\" that holds quarks together inside protons and neutrons. Understanding the difference between quark and gluon jets is crucial in particle physics because quarks and gluons behave differently when they are produced in high-energy collisions. Quark-initiated jets tend to produce fewer, more energetic particles, while gluon-initiated jets produce more particles with lower energy. Being able to distinguish between the two is important for understanding the processes happening inside the detector and for analyzing the results of particle collisions more accurately. This dataset presents a significant challenge due to the complexity and high-dimensional nature of the data. Unlike simpler image datasets like MNIST, where the patterns are more easily recognized, the Quark-Gluon Dataset requires more sophisticated models to capture the underlying structure of the particle collisions. Models like Graph Neural Networks (GNNs) , which can model relationships between particles, and Hybrid Quantum Models , which explore the potential advantages of quantum computing, were used to tackle these challenges. While traditional models like Convolutional Neural Networks (CNNs) performed well on simpler datasets, they struggled with the complexities of the Quark-Gluon data, highlighting the need for more advanced approaches. HEP PYTHIA Synthetic Particle Jets In addition to real experimental data, synthetic particle jets were generated using the PYTHIA event generator, a widely used tool for simulating particle collisions in high-energy physics. These particle jets consist of multiple physical properties for each particle in the jet, represented as 4-tuples: transverse momentum (pT), rapidity (y), azimuthal angle (ϕ), and particle ID. The task was to classify whether each jet was quark- or gluon-initiated, and this dataset further validated the model's ability to process particle-level data using GNNs and hybrid quantum models. A preprocessed version of the original dataset is also available with 12,500 jets , each converted into graph-based data with 8 features per node , capturing the following derived properties: \\(p_T\\) : Transverse Momentum — Measures how much momentum a particle has perpendicular to the collision axis (beamline). This helps in understanding the energy distribution in the transverse plane. \\(y\\) : Rapidity — Describes how the particle's velocity compares to the speed of light along the direction of motion, offering insights into the particle's energy and momentum. \\(\\phi\\) : Azimuthal Angle — The angle in the transverse plane, ranging from 0 to \\(2\\pi\\) , indicating the particle's direction relative to the beamline. \\(m\\) : Rest Mass — The intrinsic mass of the particle when it is at rest. \\(E\\) : Total Energy — The sum of the particle's kinetic and potential energy. \\(p_x\\) : Momentum along the x-axis — The x-component of the particle's momentum vector. \\(p_y\\) : Momentum along the y-axis — The y-component of the particle's momentum vector. \\(p_z\\) : Momentum along the z-axis — The z-component of the particle's momentum vector. Each jet is represented as a graph where the nodes correspond to particles, and the edges represent the relationships between them. The task was to classify these jets into two classes: quark-initiated or gluon-initiated . Quark jets tend to have fewer, more energetic particles, while gluon jets have more particles with lower energy. 3. Quantum Machine Molecular Energy (QM7) Dataset For regression tasks, I turned to the Quantum Machine 7 (QM7) dataset. This dataset consists of 7,165 molecules, each represented by its atomic coordinates and corresponding formation energy —a crucial property in quantum chemistry that reflects the stability of a molecule. Formation energy is the energy required to form a compound from its constituent atoms, and predicting this accurately is critical in areas like materials discovery and drug design. The challenge was to predict the formation energy for these molecules using contrastive learning methods. The QM7 dataset is relatively small, but it provided a perfect testing ground for the quantum variational circuits integrated into the machine learning models. By combining Graph Neural Networks (GNNs) with quantum models , I aimed to improve the model's performance in predicting molecular properties. While the Mean Absolute Error (MAE) results from the quantum models were comparable to classical methods, further optimization is required to fully explore the potential of quantum models in this domain. Contrastive Losses Implementations Contrastive losses are the core mechanism for our approach of representation learning. The implementation often involves several key components that work together to optimize the embedding space and improve the learning process, such as distances , miners , and reducers . The distance function is a crucial part of the contrastive loss. It measures the similarity or dissimilarity between embeddings in the latent space. Common choices include: - Euclidean distance : Measures the straight-line distance between two embeddings. It's often used for calculating how \"far apart\" two points are in the latent space. - Cosine similarity : Computes the cosine of the angle between two embeddings. This is especially useful when the direction of the embeddings is more important than their magnitude. Miners is the optional component but they can significantly improve the efficiency and effectiveness of the training process by identifying the most informative pairs for training. In a batch of data, not all positive or negative pairs contribute equally to the learning process. Some pairs are already well separated or clustered, so focusing on them may not improve the model much. Reducers control how the loss is calculated and aggregated across the batch. They could, additionally, apply techniques like regularization , which limits the impact of outliers so no single pair dominates the loss function, or weighted loss , which assigns different importance to pairs, ensuring that the most informative or challenging examples are prioritized during training. In a typical contrastive learning workflow: 1. Embeddings are generated by the model, with a shape of \\((N, \\text{embedding\\_size})\\) , where \\(N\\) is the batch size. 2. Distances are computed between positive and negative pairs in the embedding space. 3. Miners identify the most challenging pairs within the batch (hard positives and hard negatives). 4. Reducers aggregate the loss across the batch, ensuring that hard examples are emphasized, and regularization is applied. Through iterative updates, the model learns to create a more discriminative and robust feature space, fascilitating downstream tasks like classification or clustering. Examples While there are several types of contrastive losses, we focused on a few key variants, revisiting the writing from midterm report. Pair Contrastive Loss Pair contrastive loss is designed to minimize the distance between positive pairs and separate negative pairs by a specified margin \\(m\\) . The general form of the loss is: $$ \\mathcal{L} = \\sum_{i=1}&#94;{N} (1 - y_{ij}) \\cdot \\max(0, m - d(x_i, x_j)) + y_{ij} \\cdot d(x_i, x_j)&#94;2 $$ Where: - \\(y_{ij}\\) is a binary label (1 for positive pairs, 0 for negative pairs). - \\(d(x_i, x_j)\\) is the distance between embeddings (e.g., Euclidean or cosine). - \\(m\\) is the margin that controls how far apart negative pairs should be. Alternative distance metrics, like cosine similarity, can also be used depending on the task, each with different advantages. Temperature-Scaled Losses Temperature-scaled losses, such as NT-Xent and InfoNCE , introduce a temperature parameter \\(\\tau\\) to smooth the similarity distribution and emphasize harder negative examples. The InfoNCE loss is given by: $$ \\mathcal{L}_{\\text{InfoNCE}} = -\\sum_{i=1}&#94;{N} \\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}&#94;{N} \\exp(\\text{sim}(z_i, z_k)/\\tau)} $$ Where: - \\(\\text{sim}(z_i, z_j)\\) is the cosine similarity between embeddings. - \\(\\tau\\) controls the sharpness of the similarity distribution, making the model more sensitive to challenging examples. Alignment and Uniformity The alignment and uniformity metrics provide useful ways to evaluate how well the embedding space is structured: - Alignment loss ensures that positive pairs are closely clustered: $$ \\mathcal{L}_{\\text{alignment}} = \\mathbb{E}_{(z, z&#94;+)} \\| f(z) - f(z&#94;+) \\|_2&#94;2 $$ This metric minimizes the distance between positive pairs, improving representation quality. Uniformity loss measures how well the representations are distributed across the latent space: $$ \\mathcal{L}_{\\text{uniformity}} = \\log \\mathbb{E}_{z_i, z_j} e&#94;{-2 \\| z_i - z_j \\|&#94;2} $$ It encourages embeddings to spread out across the space, maximizing utilization of the latent space. The two losses can be combined: $$ \\mathcal{L} = a \\mathcal{L}_{\\text{uniformity}} + (1-a) \\mathcal{L}_{\\text{alignment}} $$ This helps create a balanced embedding space where positive pairs are tightly clustered while maintaining good separation among negative pairs. Quantum Fidelity In quantum computing, fidelity measures the similarity between two pure quantum states . There are two primary ways to compute this: through direct fidelity calculation or using the SWAP test . For two pure quantum states, (|\\psi_1\\rangle) and (|\\psi_2\\rangle), the fidelity is given by the squared magnitude of their inner product: $$ F(|\\psi_1\\rangle, |\\psi_2\\rangle) = |\\langle \\psi_1 | \\psi_2 \\rangle|&#94;2 $$ As full quantum states are usually not accessible, an approximation of fidelity can be computed using the Bhattacharyya coefficient : $$ F_{\\text{direct}} = \\left( \\sum_{i} \\sqrt{p_{1i} \\cdot p_{2i}} \\right)&#94;2 $$ Here, ( p_{1i} ) and ( p_{2i} ) are the probabilities of specific measurement outcomes for the two states. On the other hand, the SWAP test is a quantum algorithm that estimates fidelity by using an ancillary qubit and performing controlled-SWAP operations between corresponding qubits of the two states. The fidelity is inferred from the probability of measuring the ancillary qubit in the ( |0\\rangle ) state: $$ P(|0\\rangle) = \\frac{1 + F(|\\psi_1\\rangle, |\\psi_2\\rangle)}{2} $$ From this, the fidelity is: $$ F = 2 \\cdot P(|0\\rangle) - 1 $$ In Pennylane , these approaches can be simulated to verify the theory: Direct Fidelity Calculation : We can use qml.state() to access the full quantum states and compute their inner product directly, or qml.probs() to extract the probability distributions and approximate fidelity using the Bhattacharyya coefficient. SWAP Test : The SWAP test can be implemented with controlled-SWAP gates and an ancillary qubit to estimate fidelity based on the measurement probability of the ancillary qubit. Here, these methods are implemented and tested in Pennylane: import pennylane as qml from pennylane import numpy as np # Define quantum devices dev1 = qml . device ( 'default.qubit' , wires = 3 ) dev2 = qml . device ( 'default.qubit' , wires = 3 ) # Define two circuits @qml . qnode ( dev1 ) def circuit1 (): qml . Hadamard ( wires = 0 ) qml . CNOT ( wires = [ 0 , 1 ]) qml . CNOT ( wires = [ 1 , 2 ]) qml . Hadamard ( wires = 2 ) return qml . probs ( wires = [ 0 , 1 , 2 ]) @qml . qnode ( dev2 ) def circuit2 (): qml . PauliX ( wires = 0 ) qml . CNOT ( wires = [ 0 , 1 ]) qml . CNOT ( wires = [ 1 , 2 ]) qml . Hadamard ( wires = 2 ) return qml . probs ( wires = [ 0 , 1 , 2 ]) # return qml.state() # qml.density_matrix(wires=[0, 1, 2]) # qml.sample(qml.PauliZ(0)) # qml.expval(qml.PauliZ(0)) # Get the quantum states from each circuit state1 = circuit1 () state2 = circuit2 () print ( \"State 1 and State 2:\" ) print ( state1 , state2 ) # Compute the direct fidelity between the two states fidelity_direct = np . sum ( np . sqrt ( state1 * state2 )) ** 2 # qml.probs # fidelity_direct = np.abs(np.dot(np.conj(state1), state2)) ** 2 # qml.state() print ( f \"Direct Fidelity: { fidelity_direct } \" ) # Define a SWAP test circuit with 7 qubits (3 for each state + 1 ancillary) dev_swap = qml . device ( 'default.qubit' , wires = 7 ) @qml . qnode ( dev_swap ) def swap_test_circuit (): # Apply Hadamard on the ancillary qubit qml . Hadamard ( wires = 0 ) # Prepare the first quantum state (on qubits 1, 2, 3) qml . Hadamard ( wires = 1 ) qml . CNOT ( wires = [ 1 , 2 ]) qml . CNOT ( wires = [ 2 , 3 ]) qml . Hadamard ( wires = 3 ) # Prepare the second quantum state (on qubits 4, 5, 6) qml . PauliX ( wires = 4 ) qml . CNOT ( wires = [ 4 , 5 ]) qml . CNOT ( wires = [ 5 , 6 ]) qml . Hadamard ( wires = 6 ) # Apply controlled SWAP gates between corresponding qubits qml . CSWAP ( wires = [ 0 , 1 , 4 ]) qml . CSWAP ( wires = [ 0 , 2 , 5 ]) qml . CSWAP ( wires = [ 0 , 3 , 6 ]) # Apply final Hadamard gate on the ancillary qubit qml . Hadamard ( wires = 0 ) # Measure the ancillary qubit return qml . probs ( wires = 0 ) # Execute the SWAP test probs = swap_test_circuit () # Estimate fidelity using the SWAP test result fidelity_swap = 2 * probs [ 0 ] - 1 print ( f \"Fidelity (SWAP Test): { fidelity_swap } \" ) The output is as follow when using qml.probs : Circuit 1 : 0 : ── H ─╭●───────┤ ╭ Probs 1 : ────╰ X ─╭●────┤ ├ Probs 2 : ───────╰ X ── H ─┤ ╰ Probs Circuit 2 : 0 : ── X ─╭●───────┤ ╭ Probs 1 : ────╰ X ─╭●────┤ ├ Probs 2 : ───────╰ X ── H ─┤ ╰ Probs State 1 and State 2 : [ 0.25 0.25 0. 0. 0. 0. 0.25 0.25 ] [ 0. 0. 0. 0. 0. 0. 0.5 0.5 ] SWAP Test Circuit : 0 : ── H ──────────╭●────╭●────╭●───── H ─┤ Probs 1 : ── H ─╭●───────├ SWAP ─│─────│────────┤ 2 : ────╰ X ─╭●────│─────├ SWAP ─│────────┤ 3 : ───────╰ X ── H ─│─────│─────├ SWAP ────┤ 4 : ── X ─╭●───────╰ SWAP ─│─────│────────┤ 5 : ────╰ X ─╭●──────────╰ SWAP ─│────────┤ 6 : ───────╰ X ── H ─────────────╰ SWAP ────┤ Fidelity ( Direct Calculation ) : 0.4999999999999996 Fidelity ( SWAP Test Estimation ) : 0.4999999999999989 and qml.state : Circuit 1 : 0 : ── H ─╭●───────┤ State 1 : ────╰ X ─╭●────┤ State 2 : ───────╰ X ── H ─┤ State Circuit 2 : 0 : ── X ─╭●───────┤ State 1 : ────╰ X ─╭●────┤ State 2 : ───────╰ X ── H ─┤ State State 1 and State 2 : [ 0.5 + 0. j 0.5 + 0. j 0. + 0. j 0. + 0. j 0. + 0. j 0. + 0. j 0.5 + 0. j -0.5 + 0. j ] [ 0. + 0. j 0. + 0. j 0. + 0. j 0. + 0. j 0. + 0. j 0. + 0. j 0.70710678 + 0. j -0.70710678 + 0. j ] SWAP Test Circuit : 0 : ── H ──────────╭●────╭●────╭●───── H ─┤ Probs 1 : ── H ─╭●───────├ SWAP ─│─────│────────┤ 2 : ────╰ X ─╭●────│─────├ SWAP ─│────────┤ 3 : ───────╰ X ── H ─│─────│─────├ SWAP ────┤ 4 : ── X ─╭●───────╰ SWAP ─│─────│────────┤ 5 : ────╰ X ─╭●──────────╰ SWAP ─│────────┤ 6 : ───────╰ X ── H ─────────────╰ SWAP ────┤ Fidelity ( Direct Calculation ) : 0.4999999999999998 Fidelity ( SWAP Test Estimation ) : 0.4999999999999989 Architecture 1. Supervised and Unsupervised Contrastive Learning During this phase, I further investigated the loss functions used in both supervised and unsupervised contrastive learning (CL). This exploration extended to both full quantum and hybrid models. The key focus was to evaluate how well different loss functions, such as temperature-scaled losses like InfoNCE and pairwise contrastive losses, could perform when integrated into hybrid classical-quantum networks. In particular, I analyzed how these loss functions influence the representations learned by classical models versus those augmented by quantum circuits. Loss Function Exploration 2. Graph Processing for Quark-Gluon Jet Images For the Quark-Gluon Jet (QJ) images, I moved beyond convolutional approaches and began exploring graph-based models to better capture the underlying structure of the data. Graph Processing for QJ Images 3. Quantum Integration 4. Rationale Discovery for Graph Contrastive Learning In the final approach, I focused on model-based augmentations for unsupervised contrastive representation learning, particularly in tasks such as Quark-Gluon Jet classification and the Quantum Machine 7 (QM7) regression for predicting molecular formation energy. By generating synthetic data, I utilized model-based augmentation to better exploit unsupervised settings where direct label supervision is absent. Rationale Discovery as Graph Augmentation Rationale Generator : A model component assigns probability scores to each node, estimating its importance for the task. This helps determine which nodes will be retained for creating augmented views. Positive Sample Generation (Rationale View) : Based on the rationale generator's output, high-probability nodes are retained, creating the rationale view , a smaller graph that still preserves the core, important information about the instance. Negative Sample Generation (Complement View) : Low-probability nodes or the complement of the rationale (the set of nodes that were discarded) are used to form a negative sample , called the complement view. This forces the model to contrast and learn the important features of the graph. Graph Encoder : After generating the rationale and complement views, these two graphs are processed through the same encoder , producing two views (two sets of node embeddings). The encoder learns representations that emphasize important features in the rationale view while downplaying irrelevant ones in the complement view. Contrastive Learning : The embeddings from the two views are passed to a combined loss function, to optimize the generator rationale of graph i.e importance of a node to represent an instance. Sufficiency Loss : Ensures that the rationale (positive sample) contains enough information for the instance-discrimination task. It minimizes the negative log-likelihood of the correct prediction based on the rationale view: $$ \\mathcal{L}_{\\text{sufficiency}} = - \\log p(y \\mid G_{\\text{rationale}}) $$ Independence Loss : Ensures that the complement view (negative sample) does not contain sufficient information to predict the label. It maximizes the log-likelihood of the incorrect prediction for the complement view: $$ \\mathcal{L}_{\\text{independence}} = \\log p(y \\mid G_{\\text{complement}}) $$ Contrastive Loss : Optimizes the embeddings of the rationale-based positive samples while pushing away negative samples. The contrastive loss function is defined as: $$ \\mathcal{L}_{\\text{contrastive}} = - \\log \\frac{\\exp(\\text{sim}(z_i, z_j) / \\tau)}{\\sum_{k=1}&#94;{N} \\exp(\\text{sim}(z_i, z_k) / \\tau)} $$ Total Loss : The final objective combines all three losses to ensure the rationale captures meaningful information while the complement does not: $$ \\mathcal{L} = \\lambda_1 \\mathcal{L}_{\\text{sufficiency}} + \\lambda_2 \\mathcal{L}_{\\text{independence}} + \\lambda_3 \\mathcal{L}_{\\text{contrastive}} $$ Downstream Tasks : After the model has been trained using contrastive learning for instance discrimination, the learned graph embeddings/representation can be transferred to downstream tasks such as classification or regression . Results More results will be updated in the upcoming weeks Dataset Models Accuracy Full MNIST CNN Encoder 0.93 Full Quantum CNN 0.90 0-1 MNIST CNN Encoder 0.97 Quark-Gluon Images CNN Encoder 0.56 Hybrid Quantum CNN 0.55 Quark-Gluon Particle Jet GNN Encoder 0.76 Hybrid Quantum GNN 0.76 Supervised Contrastive Learning Dataset Models Accuracy Full MNIST Augmented + CNN Encoder 0.88 Augmented + Full Quantum CNN 0.84 0-1 MNIST Augmented + CNN Encoder 0.93 Augmented + Full Quantum CNN 0.84 Quark-Gluon Particle Jet (6 particles) Rationale Augmented + GNN 0.69 Quantum Rationale Augmented + GNN 0.68 Self-Supervised Contrastive Learning (Classification) Dataset Models MAE (kcal/mol) QM7 Rationale Augmented + GNN 34 Quantum Rationale Augmented + GNN 35 Self-Supervised Contrastive Learning (Regression) Conclusion and Future Work In summary, the project demonstrated the potential of contrastive learning, both supervised and unsupervised, applied to high-energy physics data. By integrating quantum models, we were able to explore novel approaches to contrastive learning, though classical models still hold strong in terms of performance and scalability. Moving forward, I plan to explore additional frameworks like BYOL , MoCo , and SwAV for improving performance on HEP image data, as well as fully quantum pipelines for Rationale Discovery in graph-based contrastive learning. Further improvements in the backbone encoders, such as incorporating Equivariant Graph Networks , will be key in pushing the boundaries of what we can achieve in this intersection of quantum computing and machine learning. The full code repository for this project can be found here: Quantum_SSL_for_HEP_Duy_Do . Acknowledgment I want to thank my mentors and co-mentees for their incredible support during this program. A heartfelt thanks to my mentors— Sergei Gleyzer , KC Kong , Katia Matcheva , Konstantin Matchev , Myeonghun Park . Your guidance has been crucial, and I am grateful for the knowledge and encouragement you provided. To my co-mentees, Amey Bhatuse and Sanya Nanda, thank you for the teamwork and support throughout our journey. Working together has made navigating challenges much easier and more enjoyable. I also want to recognize all the contributors in the ML4Sci GSoC community—it was inspiring to share experiences and learn from each other. Lastly, a big thank you to the GSoC organizers for their hard work in coordinating this program and fostering collaboration on such a grand scale.","tags":"blogs","url":"/blogs/end-term-report-gsoc24.html","loc":"/blogs/end-term-report-gsoc24.html"},{"title":"Midterm Report - GSoC24","text":"In this post, I would like to share the progress of my GSoC24 project Quantum Contrastive Representation Learning for High Energy Physics in ML4Sci organization. Google Summer of Code (GSoC) aims to introduce open source development by providing opportunities for new contributors to work with an open source organization on a project. My GSoC experience with ML4Sci has been incredibly challenging and rewarding: Much has been learned, and a lot more to be discovered. The code and relevant resources could be found at Github . 1. Introduction This is a Machine Learning for Science project, as indicated by the name of my organization. Our goal is to train models using data from experiments and simulations to address scientific problems. The project title \"Quantum Contrastive Representation Learning for High Energy Physics\" may sound abstract, but it encompasses well the project's scope and challenge, after taking each component individually into consideration. Quantum Quantum as in \"quantum physics\", \"quantum computing\", \"quantum machine learning\", and finally \"quantum variational algorithm\", by the level of specificity, indicates this project will strive to incorporate this novel method in the machine learning model to explore advantage of quantum computing. Contrastive Representation Learning \"Representation learning\" involves creating features from data that are useful for various tasks like classification or clustering. \"Contrastive Representation Learning\" is one approach that learns these features through contrasting similar and dissimilar data points. Instead of predicting labels, it focuses on predicting a similarity metric, using contrastive loss functions to pull similar data points together and push dissimilar ones apart. High Energy Physics (HEP) This refers to the target data domain of the project, which is particularly important consideration for representation learning. The HEP datasets used include Photon-Electron images, Quark-Gluon images, and Quark-Gluon Jet Clouds. Additionally, standard classical datasets like MNIST are employed for benchmarking and initially evaluating the models' performance. The potential scope to investigate is still, indeed, very large. We are not limited to any particular network architecture, be it MLP, CNN, or GNN, along with more advanced considerations like equivariance. Quantum circuits have also evolved into many analogs of the classical networks, with options to be incorporated in either hybrid or pure forms. In addition, contrastive learning includes both supervised and self-supervised approaches, with very different objectives and data utilities. Finally, steps such as data augmentation require not only understanding of the datasets but also trials and errors, and models working well on one domain are not guaranteed to perform on others. 2. Quantum Variational Algorithm The Quantum Variational Algorithm (QVA) is a hybrid quantum-classical approach designed to solve optimization problems. The core idea is to use a parameterized quantum circuit whose trainable parameters are adjusted by a classical optimizer. The quantum circuit prepares a quantum state, and measurements of this state are used to evaluate a cost function, which the classical optimizer then minimizes by adjusting the quantum circuit parameters. Quantum state encoding in QVAs can be done using amplitude or angle embedding. Angle embedding involves encoding data into the rotation angles of quantum gates. A classical data point \\(x\\) can be encoded into a qubit state \\(|\\psi\\rangle = \\cos(x/2)|0\\rangle + \\sin(x/2)|1\\rangle\\) . In amplitude embedding, classical data is encoded into the amplitudes of a quantum state. A classical vector \\(\\mathbf{x} = (x_1, x_2, \\ldots, x_n)\\) can be encoded into a quantum state \\(|\\psi\\rangle = \\sum_{i=1}&#94;{n} x_i |i\\rangle\\) . Thus, a quantum state with n qubits can represent \\(2&#94;n\\) states simultaneously. Quantum Variational Algorithm Quantum gates i.e the Pauli-X (NOT), Hadamard (H), Controlled-NOT (CNOT) are the building blocks of quantum circuits, creating superpositions and entanglements to enable complex computations. The gate functions (angles) are parameterized and our goal is to find the optimized values to encode the representation. 3. Contrastive Learning Objectives Contrastive learning has proven to be an effective framework for representation learning. It encompasses both supervised and self-supervised learning, characterized by the use of class labels during training. Different from other representation learning methods like autoencoders, which aim to capture the overall structure of the data for tasks such as dimensionality reduction, contrastive learning specifically aims to distinguish between instances to learn representations that are more discriminative and robust for various downstream tasks. [1] Self-Supervised Contrastive Learning aims to learn meaningful representations from unlabeled data. Positive pairs are generated by applying different augmentations to the same instance i.e different views of the same instance should be close in the representation space. Negative pairs are formed from different instances, even if they may belong to the same class, since class labels are not available during initial (pre)training. Afterwards, the learned representations are evaluated and used for downstream tasks such as linear probing or fine-tuning on a small amount of labeled data. Frameworks such as SimCLR, MoCo, and BYOL have shown the effectiveness of this approach, often matching or even surpassing supervised learning performance on various tasks. Supervised Contrastive Learning , on the other hand, includes labeled data in the training process. Positive pairs are created from different instances of the same class and negative pairs from instances of different classes. This supervised approach helps in learning representations that not only distinguish between classes but also capture intra-class variations. [2] Unlike traditional supervised learning that optimizes for class likelihoods, supervised contrastive learning optimizes for representation quality. This can lead to better generalization, especially in distributions where intra-class variability is high, and outperform likelihood predictors by providing more robust features that are useful across various tasks. However, the effectiveness can depend on specific dataset and task. Contrastive Losses Contrastive losses are the core mechanism to bring similar pairs closer and push dissimilar pairs apart in the representation space. There are a number of loss functions, however, we have studied only a few representative ones. [4] [5] Pair Contrastive Losses Pair contrastive losses aim minimize the distance between positive pairs while ensuring that negative pairs are separated by a margin \\(m\\) . The loss function \\(\\mathcal{L}\\) is defined as: $$\\mathcal{L} = \\sum_{i=1}&#94;{N} (1 - y_{ij}) \\cdot \\max(0, m - d(x_i, x_j) ) + y_{ij} \\cdot d(x_i, x_j)&#94;2$$ where: \\(y_{ij}\\) is a binary label indicating whether the pair \\((x_i, x_j)\\) is positive (1) or negative (0). \\(d(x_i, x_j)\\) is the Euclidean distance between the embeddings. \\(m\\) is the margin parameter i.e the minimum acceptable distance for negative pairs. The distance could be replaced with other metrics like the cosine similarity. Each choices of distance metrics have their pros and cons considerations. Temperature-Scaled Losses Temperature-scaled losses, such as the NT-Xent (Normalized Temperature-scaled Cross Entropy Loss) and InfoNCE (Information Noise Contrastive Estimation), introduce a temperature parameter \\(\\tau\\) to control the sharpness of the similarity distribution. For instance, the InfoNCE loss is: $$\\mathcal{L}_{\\text{InfoNCE}} = -\\sum_{i=1}&#94;{N} \\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}&#94;{N} \\exp(\\text{sim}(z_i, z_k)/\\tau)}$$ where: \\(\\text{sim}(z_i, z_j) = \\sum_{k} \\frac{z_{i,k} z_{j,k}&#94;*}{\\|\\mathbf{z_i}\\| \\|\\mathbf{z_j}\\|}\\) is the cosine similarity. \\(z_i\\) and \\(z_j\\) form a positive pair (e.g., two augmentations of the same instance). \\(z_k\\) are negative samples in the batch. \\(\\tau\\) is the temperature parameter. Alignment and Uniformity The two metrics are shown to be very effective at evaluating the quality of embeddings. The alignment loss measures how close positive pairs are in the representation space, given by: $$\\mathcal{L}_{\\text{alignment}} = \\mathbb{E}_{(z, z&#94;+)} \\| f(z) - f(z&#94;+) \\|_2&#94;2$$ where: \\(f(z)\\) and \\(f(z&#94;+)\\) are the embeddings of a positive pair \\((z, z&#94;+)\\) . Thus, it makes the positive pairs tightly clustered, improving the representation quality. The uniformity loss, on the other hand, measures the dispersion of embeddings on a hypersphere, given by: $$\\mathcal{L}_{\\text{uniformity}} = \\log \\mathbb{E}_{z_i, z_j } e&#94;{-2 \\| z_i - z_j \\|&#94;2}$$ where: \\(z_i\\) and \\(z_j\\) are representations from any two samples. This metric encourage embeddings to spread out the representations as much as possible and utilize the latent space more effectively. These two losses could be used together for same or better effect than other contrastive losses: $$\\mathcal{L} = a \\mathcal{L}_{\\text{uniformity}} + (1-a) \\mathcal{L}_{\\text{alignment}}$$ , or even alongside other losses for improved performance. Alignment and Uniformity Quantum Fidelity Loss In quantum computing, there is a relevant concept of \"Fidelity Loss\" that measures the similarity between two quantum states. This provided some promising directions: for example, how uniformity and alignment in the quantum representation space could be optimized with fidelity metrics. However, the results of further considerations and experimentations have been mixed. In fact, for two pure quantum states , the fidelity is simply the squared magnitude of their inner product: $$F(|\\psi_1\\rangle, |\\psi_2\\rangle) = |\\langle \\psi_1 | \\psi_2 \\rangle|&#94;2 = (\\text{sim}(|\\psi_1\\rangle, |\\psi_2\\rangle))&#94;2 $$ This value can be measured directly using the SWAP Test procedure, which employs an auxiliary qubit and a controlled-SWAP operation. Result of measurement on the the auxiliary qubit i.e \\(P(|0\\rangle)\\) can be used to infer the fidelity: $$P(|0\\rangle) = \\frac{1 + F(|\\psi_1\\rangle, |\\psi_2\\rangle)}{2}$$ The fidelity loss for a pair of states can be defined as: $$ L_{\\text{fidelity}} = 1 - F(|\\psi_i\\rangle, |\\psi_j\\rangle) $$ By substituting into the pair contrastive loss function, we get: $$ L_{\\text{contrastive}} = \\sum_{i=1}&#94;{N} \\left[ (1 - y_{ij}) \\max(0, m + |\\langle \\psi_i | \\psi_n \\rangle|&#94;2 - 1) + y_{ij} (1 - |\\langle \\psi_i | \\psi_p \\rangle|&#94;2) \\right] $$ This is a usable loss functions for contrastive learning. We could also formulate a function similar to the InfoNCE. However, there are actually no advantage to this approach but some disadvantages. The usage of fidelity as a metric leads to lost orientation information due to the squaring operation compared to the cosine similarity: Two vectors pointing in opposite directions will have the same fidelity, even though they are dissimilar in direction. Thus, fidelity may not capture the full geometric relationships between quantum states as effectively as cosine similarity. We would like to consider further the dynamic impact of this approach on learning. Contrastive Networks Contrastive Network (Siamese) Contrastive Networks employ a shared-weight encoder that maps input to features in a latent space. In self-supervised contrastive learning, data augmentation module is critical to generates different views of the same instance, forming positive pairs, while other instances in the batch act as negative pairs. The approach can be similar in supervised contrastive learning but with additional flexibility. While data augmentation is still often used to increase the diversity of training data, it is optional since labeled data provides natural positive and negative pairs based on class labels. Additional projection head can be attached after the encoder for downstream tasks like classification. 5. Datasets The first and arguably most important key of contrastive representation learning is data i.e how to augment, how to create pair, how to encode, etc. So far, we have experimented with all the mentioned datasets, with various degree of success and comprehensiveness. MNIST The well-known MNIST dataset contains 70,000 images of handwritten digits, each 28x28 pixels. We downscale the dimension to 16x16 and use it for initial validation of models and training objectives. MNIST Data augmentation used in our experiments include random flip, random rotations, random zoom in/out, Gaussian noise addition. These would create positive pair in self-supervised mode and also help model learn invariant features insensitive to transformations. If something doesn't work on MNIST, there must be something wrong in our theories or implementations. If it does work though, there is still no guarantee of success on HEP data. Photon-Electron Images The Photon-Electron dataset contains 498,000 images with 32x32 shape and 2-channels representing energy and timing information. The energy channel captures the energy deposited by particles, while the timing channel records the temporal interaction characteristics with detector materials. Our objective is to learn a representation that distinguishes between photon and electron interactions in these HEP experiments. We also downscale each sample to dimension of 16x16 pixels. Photon-Electron The key question is how the HEP data could be augmented. Would approach for image processing still be applicable here? Quark-Gluon Images The Quark-Gluon dataset is derived from the CMS experiment at CERN's Large Hadron Collider (LHC). It consists of 933,206 images, each with three channels and a 125x125 resolution. The channels correspond to tracks from the Inner Tracking System, energy deposits from the Electromagnetic Calorimeter (ECAL), and energy deposits from the Hadronic Calorimeter (HCAL). At this point, after some unpromising results with CNN encoder, I decided to conduct some exploratory data analysis on the HEP data: Quark Gluon The data is much sparser than classical MNIST. Furthermore, we could not really discern any geometric patterns that separate the two classes. Quark Gluon - Average Distribution However, there is a clear difference in terms of intensity. The distribution seems invariant with regard to flipping transformations but not for rotation. Quark Gluon - Difference Finally, instead of CNN Encoder, we evaluate the approach of using GNN for this dataset. With the training set of 6000, we get promising result with classifying based on the first channel. GNN Classification Result on Quark Gluon Images Quark-Gluon Particle Jet Clouds The Quark-Gluon Jet Clouds dataset consists of two million jets generated using Pythia8, with each jet containing the physical properties of several particles denoted by the 4-tuple \\((pT, y, \\phi, id)\\) . These represent the transverse momentum, rapidity, azimuthal angle, and particle ID, respectively. Each jet has a class label indicating whether it originated from quarks (1) or gluons (0). Particle clouds is a completely different type of data now. In the past, we need to address permutation invariance with either network architecture and data preprocessing/augmenting, but with usage of Graph Neural Networks (GNNs), this is no longer an issue. However, we still need to study how to augment this data correctly. One particular aspect to verify is the significance of the highest momentum particle in the jet. Experiments have shown that even with only the highest momentum particle, models can achieve good results. So we analyze how the GNN encoder performs on supervised classification tasks with varying numbers of nodes \\(k\\) sorted by momentum. Classification Result with Varying Node Number k We expect potential data augmentations for this dataset include rotation, momentum perturbation, random (or momentum priority) subsampling. 6. Methods and Results CNN Encoder - Classical/Hybrid Classical convolutional neural networks (CNNs) are used to extract features from image data. The results of self-supervised on MNIST and supervised on electron-photon dataset are as follow: Representation Result on MNIST Quantum Head Architecture Result on Photon-Electron In addition to the classical model, we also evaluate the integration of a quantum head after CNN encoder for a hybrid model. QCNN Encoder - Fidelity Loss We utilize Data-Reuploading Circuits (DRCs) [8] as the convolution kernels. Similar to Siamese network, we utilize two VQCs sharing the parameters as the encoders, and fidelity is calculated using a swap test. Quantum CNN Encoder with Fidelity Loss While observing the loss decreases, I realize that this approach is not effective. For once, the utilization of two quantum circuit encoder require double the numbers of qubit, which is highly inefficient for classical quantum simulators as qubits are the main bottleneck. 7. Challenges and Future Plan Contrastive Representation Learning applied to image augmentation has been tested and verified with CNN encoders. However, our target datasets, though in a similar format, is not conventional image data. While effective augmentations are critical for the self-supervised pretraining objective on non-labeled HEP data, defining meaningful augmentations is challenging. Another significant challenge is computing resources, as contrastive learning requires large batch sizes, further complicated by the need to evaluate each pair within the batch. Finally, rigorous baseline is needed for comparative study. Clustering-based methods, as a self-supervised learning strategy with similar goals as contrastive learning but do not rely heavily on augmentations, is likely a necessary target of comparison. Another immediate next step is to apply supervised and self-supervised contrastive learning with GNN encoders. I also plan to explore using not only graph-level features but also node-level feature contrasts as self-supervised tasks [6] . On the full quantum implementation, I am going to consider in more detail the behavior of using fidelity as a metric in quantum contrastive learning and try the alternative of calculating quantum state differences using classical contrastive loss functions. Finally, I will work toward implementing graph quantum neural network with techniques like equivariant graph embedding , GSoC projects of previous years, and other research. References [1] P. H. Le-Khac, G. Healy, and A. F. Smeaton, \"Contrastive Representation Learning: A Framework and Review,\" IEEE Access, vol. 8, pp. 193907–193934, 2020, doi: 10.1109/ACCESS.2020.3031549. [2] P. Khosla et al., \"Supervised Contrastive Learning,\" Mar. 10, 2021, arXiv: arXiv:2004.11362. doi: 10.48550/arXiv.2004.11362. [3] B. Jaderberg, L. W. Anderson, W. Xie, S. Albanie, M. Kiffner, and D. Jaksch, \"Quantum Self-Supervised Learning,\" Apr. 04, 2022, arXiv: arXiv:2103.14653. doi: 10.48550/arXiv.2103.14653. [4] F. Wang and H. Liu, \"Understanding the Behaviour of Contrastive Loss,\" Mar. 20, 2021, arXiv: arXiv:2012.09740. doi: 10.48550/arXiv.2012.09740. [5] T. Wang and P. Isola, \"Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere,\" Aug. 15, 2022, arXiv: arXiv:2005.10242. doi: 10.48550/arXiv.2005.10242. [6] R. Dangovski et al., \"Equivariant Contrastive Learning,\" Mar. 14, 2022, arXiv: arXiv:2111.00899. doi: 10.48550/arXiv.2111.00899. [7] W. Ju et al., \"Towards Graph Contrastive Learning: A Survey and Beyond,\" May 20, 2024, arXiv: arXiv:2405.11868. doi: 10.48550/arXiv.2405.11868. [8] A. Pérez-Salinas, A. Cervera-Lierta, E. Gil-Fuster, and J. I. Latorre, \"Data re-uploading for a universal quantum classifier,\" Quantum, vol. 4, p. 226, Feb. 2020, doi: 10.22331/q-2020-02-06-226.","tags":"blogs","url":"/blogs/midterm-report-gsoc24.html","loc":"/blogs/midterm-report-gsoc24.html"}]};