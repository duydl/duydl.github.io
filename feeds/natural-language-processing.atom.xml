<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ï€-Radiant Archive - Natural Language Processing</title><link href="/" rel="alternate"></link><link href="/feeds/natural-language-processing.atom.xml" rel="self"></link><id>/</id><updated>2019-05-26T00:00:00+09:00</updated><entry><title>Optimize Computational Efficiency of Skip-Gram with Negative Sampling</title><link href="/articles/optimize_computational_efficiency_of_skip-gram_with_negative_sampling.html" rel="alternate"></link><published>2019-05-26T00:00:00+09:00</published><updated>2019-05-26T00:00:00+09:00</updated><author><name>duydl</name></author><id>tag:None,2019-05-26:/articles/optimize_computational_efficiency_of_skip-gram_with_negative_sampling.html</id><summary type="html">&lt;p&gt;{% notebook downloads/notebooks/Optimizing_Computational_Efficiency_of_Skip-Gram_with_Negative_Sampling.ipynb cells[1:2] %}&lt;/p&gt;</summary><content type="html">&lt;p&gt;{% notebook downloads/notebooks/Optimizing_Computational_Efficiency_of_Skip-Gram_with_Negative_Sampling.ipynb cells[:] %}&lt;/p&gt;</content><category term="Natural Language Processing"></category><category term="data-mining"></category><category term="nlp"></category><category term="word2vec"></category><category term="word vectors"></category><category term="window size"></category><category term="skip-gram"></category><category term="neural network"></category><category term="negative sampling"></category><category term="stochastic gradient descent"></category><category term="learning rate"></category><category term="sigmoid"></category><category term="softmax"></category><category term="algorithm complexity"></category><category term="noise distribution"></category></entry><entry><title>Demystifying Neural Network in Skip-Gram Language Modeling</title><link href="/articles/demystifying_neural_network_in_skip_gram_language_modeling.html" rel="alternate"></link><published>2019-05-06T00:00:00+09:00</published><updated>2019-05-06T00:00:00+09:00</updated><author><name>duydl</name></author><id>tag:None,2019-05-06:/articles/demystifying_neural_network_in_skip_gram_language_modeling.html</id><summary type="html">&lt;p&gt;{% notebook downloads/notebooks/demystifying_neural_net_algorithm_behind_word2vec_skip_gram.ipynb cells[1:2] %}&lt;/p&gt;</summary><content type="html">&lt;p&gt;{% notebook downloads/notebooks/demystifying_neural_net_algorithm_behind_word2vec_skip_gram.ipynb cells[:] %}&lt;/p&gt;</content><category term="Natural Language Processing"></category><category term="data-mining"></category><category term="nlp"></category><category term="word2vec"></category><category term="co-occurrence matrix"></category><category term="vector space model"></category><category term="word vectors"></category><category term="window size"></category><category term="skip-gram"></category><category term="neural network"></category><category term="negative sampling"></category><category term="stochastic gradient descent"></category><category term="learning rate"></category></entry><entry><title>Understanding Multi-Dimensionality in Vector Space Modeling</title><link href="/articles/understanding_multi-dimensionality_in_vector_space_modeling.html" rel="alternate"></link><published>2019-04-16T00:00:00+09:00</published><updated>2019-04-16T00:00:00+09:00</updated><author><name>duydl</name></author><id>tag:None,2019-04-16:/articles/understanding_multi-dimensionality_in_vector_space_modeling.html</id><summary type="html">&lt;p&gt;{% notebook downloads/notebooks/Understanding_Multi-Dimensionality_in_Vector_Space_Modeling.ipynb cells[1:2] %}&lt;/p&gt;</summary><content type="html">&lt;p&gt;{% notebook downloads/notebooks/Understanding_Multi-Dimensionality_in_Vector_Space_Modeling.ipynb cells[:] %}&lt;/p&gt;</content><category term="Natural Language Processing"></category><category term="data-mining"></category><category term="nlp"></category><category term="dimensional reduction"></category><category term="svd"></category><category term="tf-idf"></category><category term="word2vec"></category><category term="co-occurrence matrix"></category><category term="vector space model"></category><category term="word vectors"></category><category term="3D"></category><category term="nltk"></category><category term="tokenization"></category><category term="window size"></category></entry></feed>